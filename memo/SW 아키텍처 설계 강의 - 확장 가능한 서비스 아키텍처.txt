[읽기/쓰기 서비스]
기본적으로 읽기/쓰기 서비스는 무조건 나눠야 한다.
읽기/쓰기 서비스를 나눌때 프로세스 단위로 나누는가?
	-> 배포 단위로 나눈다. 도커, 쿠버네티스 개념에서는 하나의 프로세스로 볼 수 있지만
	현업에서는 운영에서 도커, 쿠버네티스에 대한 저항이 있어 배포 단위로 분리되는 경우가 흔함.
	운영팀이 주로 사용하는 명령어가 도커, 쿠버네티스 환경에서 정상적으로 동작하지 않기 때문.
	쿠팡의 경우 1도커 1VM 정책을 고수하고 있음.

[파티셔닝]
horizontal partitioning = sharding
클라우드를 쓴다는 것은 공유 자원을 쓴다는 것
	-> 갑자기 어디선가 요청이 많이 들어와서 성능이 확 나빠진다.
	특히 IO 요청이 많아지면 성능이 확 나빠진다.
	따라서 하나의 테이블에 IO 가 많아지면 테이블을 수평적으로 쪼개야 한다.
	-> IOPS(I/O per seconds) 와 Queue length 를 관리해야함
	IOPS 가 낮고 Queue length 가 많아진다는 건 IO 요청이 많아지고 있어
	성능이 매우 낮아지고 있다는 지표.
	-> 디스크 IO 가 많은 서비스를 클라우드에 올리면 비용이 어마어마하게 늘어난다.
	그런 서비스는 IDC 에 넣는 것이 더 낫다
	-> 디스트 IO 를 관리하는 것은 매우 중요하다

[Key or Hash Based Partitioning]
대표적으로 몽고디비
	몽고디비는 scalability(Auto-sharding) 를 기본적으로 지원함
	mongos 에 던지기만 하면 read/write 를 알아서 나눠서 동작함
	내부적으로 샤딩이 되어있고 watchdog 이 있고 failover 메커니즘도 있음
	mysql, mariadb 는 오토 샤딩 개념이 없어 직접 만들어야함
	
[프록시]
	포워드 프록시 : 나가는 것을 분배할 때 사용
	리버스 프록시 : 들어오는 것을 분배할 때 사용

[메시지큐]
	모니터링 서비스를 만들때는 메시지 큐가 필수
		보통 3개의 메시지 큐를 클러스터로 묶고 메시지 큐 앞 단에
		read/write API Server 가 있음
		write API Server 가 메시지 큐에 데이터 쓰기 요청을 넣고
		클라이언트에게는 바로 200 응답
		메시지 큐 뒷단에는 컨슈머가 있고 컨슈머가 큐에서 데이터를 꺼내서 들고가는 구조
	DB 에 데이터를 쓰는 작업인 경우 
		데이터를 들어올때마다 DB 에 쓰면 테이블에 락이 걸림
		디비 부하도 매우 심해지고
		이럴 때 DB 쓰기 요청을 레디스 같은 메시지 큐에 쌓고
		n초 또는 n분 동안 DB 에 쓸 데이터를 모아서 한 번에 처리
		이렇게 하면 DB 요청을 많이 줄일 수 있고 데이터 손실도 없음
	꼭 봐야할 책 : The Architecture of Open Source Applications

[데이터베이스]
	mysql - innoDB
		read, write caching
		메모리가 있고 메모리에서 사용자 요청을 처리하고 시간이 되면 디스크에 적용함
	MSSQL
		innoDB 와 다른 구조
		하나의 테이블에 3개의 행 이상이 락이 걸리면 전체 테이블에 락이 걸림
		-> myssql 처럼 쓰면 바로 장애가 남
	CAP
		C : 모든 노드가 같은 시간에 같은 데이터를 보여줘야 한다
		A : DB 에 대한 모든 read, write 동작은 항상 성공적으로 반환
		P : 노드간 통신 문제가 생겨 메시지를 주고받지 못하는 상황에도 동작해야 한다
		CA : 시스템이 죽더라도 메시지 손실은 방지 - RDBMS
		CP : 모든 노드가 함께 퍼포먼스를 내야한다 - mongo, redis
		CA : 비동기화된 서비스 스토어 - Dynamo
		NoSQL : 엄청난 데이터가 생기며 데이터베이스에 대한 수평적 확장 
			- 옆에 서버 한 대 더 배치해서 데이터베이스를 늘리고 싶다) 
			-> P 를 충족시키고 싶다
		
[물류 대시보드]
	write 많은 서비스 -> OLAP 아키텍처 사용
		많은 데이터를 지역 DB 로 분배
		데이터를 읽을 때는 최근 데이터 위주로 다루기 때문에 레디스 캐시 서버를 사용
		대용량 데이터를 활용해 1분, 5분, 30분 요약 데이터를 
		어떻게 빨리 만들 것인지가 핵심임

[DevOps]
	배포할게 많아지면 어떻게 관리할 것인가?
	젠킨스로 빌드 하고 
	-> HashCorp Packer 로 도커, 쿠버네티스 또는 클라우드에 맞게 이미지를 굽고
	-> HashiCorp Terraform 으로 배포를 하고
	-> Canary 를 통해 일부만 배포를 해서 기존 서버와 비교를 하며 점진적으로 배포하고
	-> 문제 없으면 Global Release
	
[쿠팡 아키텍처]
	Common Storage 와 Cache 의 데이터 일관성을 위해
	Notification Queue 와 Cache Invalidator 를 사용하는데,
	Common Storage 의 데이터가 수정이 되고 Cache 에 반영되기 전 사이의 시간에
	Read 요청이 오면 데이터 일관성이 깨지지 않나요?
	
	이런 경우 Read 는 Common Storage 에서 직접 읽지 않고 
	캐쉬를 통해서만 하도록 해 유지를 하나요?
	
	-> 
	저의 생각,
	Trade Off 관계로 보시면 됩니다.
	
	일관성이 얼마만큼 중요하냐의 기준에 따라 달라집니다.
	일관성을 정확히 지켜야 되는 서비스라면, consistency 를 지키기 위해 transaction 등을 통해 지켜야 될 듯 합니다.
	
	다만 말슴드린 것과 같이 대부분의 서비스는 near realtime consisteny 를 유지하는 것으로
	대부분 만족 합니다.
	즉, 잘못된 값을 가지고 들어오더라도, 맨 마지막에 계산을 할 대 상품값이 변경 되었다거나 등등으로 다시 한 번 체크해서 보자을 하면 됩니다.
	
	모든 것을 넘 강력한 tx 로 관리하는 비용이 넘 커서 결재 이외에 이 범주를 잘 설정하는 것이 중요합니다.
	
	동기화 영역을 한정해서 동기화 할 지 -> 배그처럼
	비동기화 해서 진행을 하고 가끔 내가 필살기를 썼는데도 상대방이 다시 원복되는 상황 -> 포켓몬고
	
	이 부분은 비즈니스의 범주에 맞게 결정을 해야되는 문제가 있습니다.
	
	그리고 캐쉬서버가 여러대 인 경우 캐쉬서버 같 데이터 동기화는 어떻게 보장하나요?
	요구사항에서 실시간으로 꼭 맞을 필요는 없다고 정의해 처리를 할까요??
	
	->
	후자에 가까울듯 합니다.
	
	다만 여러대 인 경우 동기화를 하는 경우보다 트래픽 처ㅣ량이 이슈가 되므로 
	그럴 때는 cluster 를 이용하면 됩니다.
	https://www.daddyprogrammer.org/post/1601/redis-cluster/
	마치 db master-slave 과 컨셉이 동일합니다.
	
	다만 이것도 도메인 (분야 -결재, 회원가입 등) 별로 대응을 해야 겠지요.
	저현 다른 도메인을 하나의 캐시로 두면 안된다는 전제가 갈려 있습니다.
	
[우버 아키텍처]
	웹소켓???? 클라이언트 서버 동기화??